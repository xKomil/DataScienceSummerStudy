## Machine learning is great for:
- Problems for which solutions require a lot of fine-tuning or long list of rules
- Complex problems for which using a traditional approach yields no good solutions
- Fluctuating environments 
- Getting insights about complex problems and large amounts of data


# Types of machine learning systems:
- how they are supervised during training (supervised, unsupervised, semi-supervised, self-supervised, reinforcement learning etc)
- whether or not they can learn incrementally on the fly (online versus batch learning)
- Whether they work by simply comparing new data points to known data points, or instead by detecting patterns in the training data and building a predictive model, much like scientists do (instance-based versus modelbased learning)

# Supervised learning
The training data is labeled, you give model desired solutions. Typical task for supervised learning is classification, predicting a target numeric value called regression.

# Unsupervised learning
The training data is not labeled, you give model a set of data and it tries to find patterns. 
You use clustering algorithm, visualization algorithms, dimensionality reduction, feature extraction, anomaly detection, novelty
detection,  association rule learning. 

# Semi-supervised learning
The training data is partially labeled, you give model a set of data and it tries to find patterns
For example recognizing poeple in photos

# Self-supervised learning
The training data is unlabeled, you give model a set of data and it tries to find patterns

# Reinforcement learning
The learning system called agent, can observe the environment, select and perform actions, and get rewards in return or penalties. It must learn what is the best strategy called a policy, to get the most reward over time. 
Robots implement reinforcement learning algorithms to learn how to walk. 

# Batch Versus Online Learning
Batch learning is when you train the model on the entire training data set at once. Its called offline learning.
Online learning is when you train the model on a single training example at a time or in small groups. Its called incremental learning. 
Important parameter of online learning is how fast they should adapt to changing data - its called the learning rate. 

# Instance-Based Versus Model-Based Learning
Instance-based learning: the system learns the examples by heart, then generalizes to new cases by using a similarity measure to compare them to the learned examples
![alt text](image-2.png)
Model-based learning: Model-based learning involves creating a mathematical model that can predict outcomes based on input data
![alt text](image-1.png)


## The most common problems are either bad model or bad data
# Bad data:
- insufficient quantity
- nonrepresentative training data - data need to be representative for the cases you want to generalize to
- poor quality data - full of errors, outliners and noise
- Irrelevant features - enough relevant features and not too many
# Bad model:
- Overfitting the training data - model does well on training data but badly on new ones
Constraining a model to make it simpler and reduce the risk of overfitting is called **regularization**
The amount of regularization to apply during learning can be controlled by a **hyperparameter**. It is set prior to training and remains the same. 
- Underfitting the training data - model performs poorly on training data
Main options to fix this are: select model with more parameters, feed better features, reduce the constraints on the model
![alt text](image.png)


## Testing and validating
Splitting data into train and test commonly 80/20. 
The error rate on new cases is called the generalization error or out-of-sample error, by evaulating on test set you get estimate of error. 

# Hyperparameter tuning and Model selection
Holdout validation involves setting aside a portion of the training data as a validation set to evaluate multiple models and select the best one. 
After choosing the best model, it is trained on the full training set, including the validation set, and then evaluated on a test set to estimate generalization error. 
If the validation set is too small, model evaluations may be imprecise; if too large, the remaining training set is reduced, potentially affecting model performance. 
Repeated cross-validation, which averages performance across multiple small validation sets, provides a more accurate evaluation but increases training time. 
This method balances the need for precise evaluations with the practical constraints of training time.

# Data Dismatch
When training a model, the validation and test sets must represent the data expected in production. 
For example, if building a flower identification app, web images may not match app-taken photos. 
Therefore, validation and test sets should only include representative pictures. 
If the model performs poorly on the validation set after training on web images, the issue might be due to overfitting or data mismatch. 
To address this, a "train-dev" by Andrew Ng set can be used to diagnose overfitting. 
If overfitting is not the issue, preprocessing web images to resemble app photos might help. Finally, evaluate the model on the test set to estimate production performance.

## Summary
- Machine learning is about making machines get better at some task by
learning from data, instead of having to explicitly code rules.
- There are many different types of ML systems: supervised or not, batch
or online, instance-based or model-based.
- In an ML project you gather data in a training set, and you feed the
training set to a learning algorithm. If the algorithm is model-based, it
tunes some parameters to fit the model to the training set (i.e., to make
good predictions on the training set itself), and then hopefully it will be
able to make good predictions on new cases as well. If the algorithm is
instance-based, it just learns the examples by heart and generalizes to
new instances by using a similarity measure to compare them to the
learned instances.
- The system will not perform well if your training set is too small, or if
the data is not representative, is noisy, or is polluted with irrelevant
features (garbage in, garbage out). Lastly, your model needs to be
neither too simple (in which case it will underfit) nor too complex (in
which case it will overfit).




TIP
It is often a good idea to try to reduce the number of dimensions in your training data
using a dimensionality reduction algorithm before you feed it to another machine learning
algorithm (such as a supervised learning algorithm). It will run much faster, the data will
take up less disk and memory space, and in some cases it may also perform better.

NOTE
Transferring knowledge from one task to another is called transfer learning, and itâ€™s one
of the most important techniques in machine learning today, especially when using deep
neural networks

TIP
Overfitting happens when the model is too complex relative to the amount and noisiness
of the training data. Here are possible solutions:
Simplify the model by selecting one with fewer parameters (e.g., a linear model
rather than a high-degree polynomial model), by reducing the number of attributes
in the training data, or by constraining the model.
Gather more training data.
Reduce the noise in the training data (e.g., fix data errors and remove outliers).