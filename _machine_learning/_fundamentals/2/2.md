# Step by step project as Data scientist
## Steps
- Look at the big picture 
- Get the data
- Explore and visualize the data to gain insights
- Prepare the data for machine learning algorithms
- Select a model and train it
- Fine-tune your model
- Present your solution
- Maintain your system

# Look at the big picture
- What is the problem you are trying to solve?
- What is the business goal?
- Select a performance measure
- Check the assumptions


A *pipeline* in machine learning refers to a sequence of data processing steps, where the output of one component is the input for the next. This structured flow ensures the systematic handling of data transformations, model training, and evaluation.

### Business goal / problem
In this example, the goal is to predict the median housing price in any district based on various metrics.
It's typical supervised learning task, because we can train the model with labeled examples (we have expected median housing price). 
It's a regression task, moreover its a multiple regression problem cause we use multiple features to make a preduction. 
It's a univariate regression since we trying to predict one value for each district. If it were to predict multiple value it would be a multivare regression. 

### Performance measure
[Ocena modeli][https://home.agh.edu.pl/~pszwed/wiki/lib/exe/fetch.php?media=med:med-w03.pdf]
*RMSE* - standardowy błąd regresji/estymaty
*MAE* - średni błąd regresji/estymaty

# Get the data

Download the data and then take a look at the data structure
atribute = column
Create a histogram look for capped data, look for values they are expressed

## **Create a test set**
Split the data into training and test sets

*Data snooping* bias occurs when a dataset is used more than once for model selection, training, or evaluation, leading to overly optimistic performance estimates and potentially misleading results. This bias arises from the leakage of information between the training and evaluation phases, which can inflate the apparent performance of a model.

Creating a test set involves setting aside a portion (typically 20%) of the dataset. While simply picking instances randomly works, it can lead to different test sets on each run, potentially exposing the entire dataset over time. To maintain consistency, you can save the test set after the first run or set a random seed to ensure the same shuffled indices. However, these methods fail if the dataset is updated. A more robust solution is using each instance's unique identifier to determine inclusion in the test set, ensuring stability even with refreshed datasets. This approach keeps the test set consistent across runs and updates.

Possible ways to split data: 

from zlib import crc32
def is_id_in_test_set(identifier, test_ratio):
return crc32(np.int64(identifier)) < test_ratio * 2**32
def split_data_with_id_hash(data, test_ratio, id_column):
ids = data[id_column]
in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
return data.loc[~in_test_set], data.loc[in_test_set]

lub:

housing_with_id = housing.reset_index() # adds an `index` column
train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "index")

lub:

housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "id")

lub:

from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

Random sampling is generally fine for large datasets, but for smaller datasets, it can introduce significant sampling bias. To ensure a representative sample, stratified sampling is used, where the population is divided into homogeneous subgroups (strata), and instances are sampled from each stratum to match the population proportions. For example, in a survey, 51.1% females and 48.9% males should be included to reflect the US population accurately. If the median income is important for predicting housing prices, you need to create income categories, ensuring each stratum has enough instances to avoid bias.

# Explore and visualize the data

- Visualize geographical data
- Look for correlations
- Experiment with attribute combinations


# Summary
Pipeline: A structured sequence of data processing steps.
Supervised Learning: Training with labeled examples.
Regression Task: Predicting continuous values.
Multiple Regression: Using multiple features for prediction.
Univariate Regression: Predicting a single value for each instance.
Multivariate regression: Predicting multiple outcome variables based on multiple predictor variables.


#120