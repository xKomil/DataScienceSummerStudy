# Step by step project as Data scientist
## Steps
- Look at the big picture 
- Get the data
- Explore and visualize the data to gain insights
- Prepare the data for machine learning algorithms
- Select a model and train it
- Fine-tune your model
- Present your solution
- Maintain your system

# Look at the big picture
- What is the problem you are trying to solve?
- What is the business goal?
- Select a performance measure
- Check the assumptions


A *pipeline* in machine learning refers to a sequence of data processing steps, where the output of one component is the input for the next. This structured flow ensures the systematic handling of data transformations, model training, and evaluation.

### Business goal / problem
In this example, the goal is to predict the median housing price in any district based on various metrics.
It's typical supervised learning task, because we can train the model with labeled examples (we have expected median housing price). 
It's a regression task, moreover its a multiple regression problem cause we use multiple features to make a preduction. 
It's a univariate regression since we trying to predict one value for each district. If it were to predict multiple value it would be a multivare regression. 

### Performance measure
[Ocena modeli][https://home.agh.edu.pl/~pszwed/wiki/lib/exe/fetch.php?media=med:med-w03.pdf]
*RMSE* - standardowy błąd regresji/estymaty
*MAE* - średni błąd regresji/estymaty

# Get the data

Download the data and then take a look at the data structure
atribute = column
Create a histogram look for capped data, look for values they are expressed

## **Create a test set**
Split the data into training and test sets

*Data snooping* bias occurs when a dataset is used more than once for model selection, training, or evaluation, leading to overly optimistic performance estimates and potentially misleading results. This bias arises from the leakage of information between the training and evaluation phases, which can inflate the apparent performance of a model.

Creating a test set involves setting aside a portion (typically 20%) of the dataset. While simply picking instances randomly works, it can lead to different test sets on each run, potentially exposing the entire dataset over time. To maintain consistency, you can save the test set after the first run or set a random seed to ensure the same shuffled indices. However, these methods fail if the dataset is updated. A more robust solution is using each instance's unique identifier to determine inclusion in the test set, ensuring stability even with refreshed datasets. This approach keeps the test set consistent across runs and updates.

Possible ways to split data: 

from zlib import crc32
def is_id_in_test_set(identifier, test_ratio):
return crc32(np.int64(identifier)) < test_ratio * 2**32
def split_data_with_id_hash(data, test_ratio, id_column):
ids = data[id_column]
in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
return data.loc[~in_test_set], data.loc[in_test_set]

lub:

housing_with_id = housing.reset_index() # adds an `index` column
train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "index")

lub:

housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "id")

lub:

from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)

Random sampling is generally fine for large datasets, but for smaller datasets, it can introduce significant sampling bias. To ensure a representative sample, stratified sampling is used, where the population is divided into homogeneous subgroups (strata), and instances are sampled from each stratum to match the population proportions. For example, in a survey, 51.1% females and 48.9% males should be included to reflect the US population accurately. If the median income is important for predicting housing prices, you need to create income categories, ensuring each stratum has enough instances to avoid bias.

# Explore and visualize the data

- Visualize geographical data
- Look for correlations
- Experiment with attribute combinations

Skewed-right distributions might require transformation logarithm or square root
Before preparing data for machine learning, consider creating new attribute combinations, such as rooms per household, bedrooms to rooms ratio, and population per household. These new attributes can provide more meaningful insights

# Prepare the Data for ML algorithm

Prepare your data for machine learning algorithms by writing functions for data transformation instead of doing it manually. This approach lets you easily reproduce transformations on any dataset, build a reusable library of functions, apply them in live systems, and experiment with different transformation combinations to find the best one.

- Data cleaning

Most machine learning algorithms can’t handle missing features, so you need to address them. For example, with missing values in the total_bedrooms attribute, you have three options:

Remove the affected districts.
Eliminate the attribute entirely.
Impute missing values by setting them to a specific value (e.g., zero, mean, median) its called *imputation*

- Handling Text and Categorical Attributes

One issue with representing categorical values numerically is that ML algorithms may assume that two nearby values are more similar than two distant values. This can be problematic for categories like ocean_proximity, where, for example, categories 0 and 4 are more similar than categories 0 and 1.
To address this, you can use one-hot encoding: create a binary attribute for each category. For instance, one attribute will be 1 for "<1H OCEAN" and 0 otherwise, another for "INLAND," and so on. This ensures that only one attribute is "hot" (1) at a time, while the others are "cold" (0).
Scikit-Learn's OneHotEncoder class can be used to convert categorical values into one-hot vectors.

A sparse matrix is ideal for matrices with mostly zeros, as it stores only nonzero values and their positions, saving memory and speeding up computations. When you one-hot encode a categorical attribute with many categories, the resulting matrix will be large and sparse, containing mostly zeros with a single 1 per row. Using a sparse matrix in this scenario is efficient.
You can work with a sparse matrix similarly to a 2D array. To convert it to a dense NumPy array, use the toarray() method.

- Feature Scalling and Transformation

Feature scaling is crucial in machine learning as algorithms often perform poorly when numerical attributes have different scales. For example, in housing data, the number of rooms ranges from about 6 to 39,320, while median incomes range from 0 to 15. Without scaling, models might ignore median income and focus more on the number of rooms.
To scale features, use one of these common methods:
- Min-Max Scaling: Rescales data to a fixed range, usually [0, 1].
- Standardization: Centers data around the mean with a unit variance

Standarization is less afected by outliners

If the target distribution has a heavy tail, you might replace the target with its logarithm. Consequently, the regression model will predict the log of the median house value, and you’ll need to compute the exponential of the model’s prediction to get the median house value.

A simpler option for transforming target values is to use a TransformedTargetRegressor. This approach involves constructing the regressor with the regression model and the label transformer, then fitting it on the training set with the original unscaled labels. It will automatically scale the labels, train the regression model on the scaled labels, and use the transformer to inverse-transform predictions.

# Custom Transformers

It’s often beneficial to transform features with heavy-tailed distributions by replacing them with their logarithm, assuming the feature is positive and the tail is on the right.

Although Scikit-Learn offers many useful transformers, you may need to create your own for custom transformations, cleanup tasks, or combining specific attributes.

FunctionTransformer is great for simple transformations, but for a trainable transformer that learns parameters in fit() and uses them in transform(), you need to create a custom class. This class should have fit(), transform(), and optionally fit_transform() methods. You can simplify this by inheriting from TransformerMixin for fit_transform() and from BaseEstimator for get_params() and set_params() methods, which help with hyperparameter tuning


# Summary
Pipeline: A structured sequence of data processing steps.
Supervised Learning: Training with labeled examples.
Regression Task: Predicting continuous values.
Multiple Regression: Using multiple features for prediction.
Univariate Regression: Predicting a single value for each instance.
Multivariate regression: Predicting multiple outcome variables based on multiple predictor variables.

TIP: For more advanced imputation methods, you can use the sklearn.impute package:
KNNImputer: Replaces missing values with the mean of the k-nearest neighbors' values, using distances based on all available features.
IterativeImputer: Uses a regression model to predict missing values based on other features, iteratively refining the model and imputed values.

Scikit-Learn Design Principles:
- Consistency: Scikit-Learn features a simple and consistent interface across its objects.
- Estimators: Any object that estimates parameters from a dataset is an estimator (e.g., SimpleImputer). Estimation is done via the fit() method, with hyperparameters set as instance variables.
- Transformers: Some estimators can also transform data (e.g., SimpleImputer). Transformation is done using the transform() method, and the fit_transform() method combines fit() and transform(), often with improved performance.
- Predictors: Estimators that make predictions (e.g., LinearRegression) use the predict() method and can assess prediction quality with score().
- Inspection: Hyperparameters are accessible via public instance variables (e.g., imputer.strategy), and learned parameters via public instance variables with an underscore suffix (e.g., imputer.statistics_).
- Nonproliferation of Classes: Datasets are handled as NumPy arrays or SciPy sparse matrices, and hyperparameters are standard Python types.
- Composition: Reuses existing components, such as creating a Pipeline from a sequence of transformers and a final estimator.
- Sensible Defaults: Provides reasonable default values for most parameters, facilitating quick baseline setups.


TIP: For categorical attributes with many categories (e.g., country code, profession), one-hot encoding can create a large number of features, potentially slowing down training and degrading performance. Instead, consider:
- Using Numerical Features: Replace the categorical feature with related numerical features (e.g., replace ocean_proximity with distance to the ocean or country code with population and GDP).
- Using Alternative Encoders: Utilize encoders from the category_encoders package on (GitHub)[https://github.com/scikit-learn-contrib/category_encoders].
- Using Embeddings: In neural networks, replace each category with a learnable, low-dimensional vector (embedding). This is a form of representation learning

WARNING: Fit scalers only to the training data using fit() or fit_transform(). After fitting, use the scaler to transform() the validation set, test set, and new data. If new data contains outliers, they might be scaled outside the range. To prevent this, set the clip hyperparameter to True.


# 139